---
title: "Productsbolt is nearly ready"
date: "2026-02-11T22:42:35.100Z"
image: /assets/blogpost-images/pretty-optimised-sunset.jpg
authorImage: /assets/alan-profile-picture.jpg
name: "Alan Reid"
category: "Project"
description: "It's pretty optimised now, but is that enough? Go faster"
---

I actually on the plane but when I got back home, I had figured out more and more to the point, it was already out of date so here I am now! I've been back for a few days, came back on the 8th of Feb and yea, I thought I had pretty much got Productsbolt sorted and I had, but I wanted to get it even better. In the last few minutes, I did the last few things needed to get it to a great point and now I can actually be content.

### Things moved and I can barely remember what I did xD

Something urked me. I know when I was in Norway, I was able to get candidate pages blacklisted in a good way and that was solid. What annoyed me was realising i could be removing good pages which needs to be on Productsbolt, which took me hours to figure out a sustainable way to go about this. AI didn't really help much, so I'm left thinking that if we're using AI Agents to get things done, I'm in the belief we're not doing it at the best way. That's another conversation and I really have to get using either OpenCode or ClaudeCode. I eventually figured mixedSignals would be a good one. If a page has mixed signals, Productsbolt can have issues and it'll go into candidate pages even if it's correct. This gives me room to go through a few passes. If the product is `editionMatch === true`, we don't want to be looking at all of the pages which hit that critieria. As you know, I then added `priceMatch === true` and this does well, but what if a surprise sale comes and I hit the page then, I'm fairly screwed. This is where `hasMixedSignals` is the last edge case. If the page comes off as being something else, but has signals which would trick the bot, but it is of the same edition of the product I am looking to track, then we'll catch it. This is a big one because it's unlikely a product will pass Productbolt's general check and then these two eyeball checks. These eyeball checks are pretty much conducted in seconds so it's not much more work, but it's a lot of pressure put off.

With this vigilence, it becomes possible to eyeball `editionMatch === true` on it's own, as there's really little to look at, and you become really confident that the bot knows what's going. With this, candidatePages went down from the high thousands, to 100-200 candidate pages. What was taking about 9 mins to scan, is now down to 35 seconds. Huge difference

### There's more webPages than candidatePages

So I did do a lot more stuff but this one was annoying. I have a lot more confirmed webPages and they are verified. That means I have to look at them. Every 2 hours, we update webPages which aren't in high priority but products that are, are done every 5 mins. This means we have to be able to clear this queue as fast as possible and manage resources. Note, I'm still working on this but it's a lot better, or I think it is at least. There's a couple of things I need to state. Some websites have lots of protection or don't act well via a normal request. As a quick solution, I was putting them into headful mode. This made sure the pages were loading and all was grand. The problem is, I've added more and more shops, more candidate pages are getting added correctly, which means we're checking more pages which potentially aren't headless requests. This takes time and effort.

The first thing was to identify which sites needed to use a Puppetter but in headless mode. After enough 403 errors, it's pretty obvious, even some 429s. I set those pages to have headless chrome. The rest stay as headful. This did help a bit, resource wise at least and it was good. I could have just exited but I wanted a little. One thing I had been doing all of this time, was using seperate Chrome instances per page. I didn't this mattered much but in some sense, I did know it could be a good place to fix things. I decided to keep all headless chrome instances on one browser. This does help and I'll work on this more and more as I might need two and round robin between the two. For now, I'm still allowing headful cases to load up their own browser. It might be the case in the future I force them to load from one browser which is headful. I'm not too sure but I'll work on this more in the future.

This did speed things up but I'm still getting resource hickups. I'm not sure if that's to do with my GPU as I'm forcing a lot of jobs to be handled at once. That's an edge case situation I'll look at but candidatePages are loading up quicker and it's getting through the high priority pages a little faster now. That little faster matters in the grand scheme of things. I don't think there's that much to optimise in this arena, so I decided to add one more thing.

### Scaling horizontally

The laptop the laptop ahhh the laptop! The GPU in it would be good enough and I will get LM Studio setup on it and use Qwen3 4B Instruct 2507 @ Q8_0. Yes it's slower but any help is good help. I'll probably work on this today as it seems fun to work on. Will figure a way to automatically get this setup with Docker in the future also but for now, it's a little bit of tinkering just to get things working. We don't need to talk much about the LM Studio stuff now, I'll speak of what's going on now.

Previously, after retriving the information on the page, I was directly ignoring the await call which then went to a module which controlled the ai workloads. This works as long as the machine is going to do the AI workloads. In the case of the laptop, it's not. It's simply trying to get through as many pages as possible to throw to the main PC. This wasn't too much of a bother as I'd already handled this with determining webpages/candidate pages but elected not to do it for updating pages. That changed! All paths that lead to wishing to use AI workloads are fired into a queue specifically to queue those jobs. I can then set the laptop to not open that channel or listen to that queue at all, thus the laptop only does what it needs to do. Processing the longer stuff at the moment which is getting through websites. This will lead to the ability to having VMs do this work. Brilliant. It's a small thing but scaling is now normalised in Productsbolt.

### The most low budget I'll get this

Productsbolt is at a very cheap point. Everything works locally, models are loaded (or will be) on LM setups, either vLLM or LM Studio, to parse and confirm the state of pages. This is all powered via just my electricity and that is that. This is purely using Qwen3 4B Instruct 2507, and the speed hinges on it. It's smart enough and fast enough to make this all work. If a smarter and faster model for 4B comes, I'm jumping on that. However right now, I'm convinced this is the best model for what the workloads I'm working on. I'm very happy with this as it's liberating to know it can work. Yes on a laptop it might be quite slow but being slow and not working at all are totally different things. On a laptop, I can (or will be able to) do what I can on the PC, but on a smaller setup.

If I could locally do everything on this setup through the PC, I would and I'm honestly going to try it. There's security issues though which I'm not sure if I want to deal with, but it might be a good learning experience. Essentially, I need a way to have a url/domain which is linked up to my IP and then communicate that way. I assume this is slow which is why I might throw this idea out of the way. Right now, it's the API, RabbitMQ and PostgreSQL needing to be hosted online. The API can eat up a lot of resources and so that's one challenge. There's somethings to learn but if those things are online, I think everything else is pretty solid. Local workers I control point to RMQ and the API and that's it, we're good.

This would allow for very small workers from VM sources to grab pages. This might take a bit of learning and fixing as I'm guessing IPs from databases will be blacklisted immediately. It's also going to be on Linux so I'll need to get sorted with that again. Regardless, it's a fun challenge but right now, isn't the biggest concern as I have plenty of machines I can use which have GPU compute. Interesting.

### Model testing

I need to get on the ball with this. As with any business, the goal is to make money. That money I'd like to be pure profit but as stated, there are some costs that need to be dealt with. Do I want LLMs to be part of that cost? I'm not sure but I can predict the cost ahead of time, while leveraging what machines I need to reduce costs as much as possible. That being said, there are instances where a more powerful LLM could be useful, and we have plenty of options nowadays to choose from. One thing that would be awesome for internal use cases, is benchmark in a simple true/false way, what jobs each LLM can do and the cost and determine how cheap an LLM can be, to complete all jobs with 0 issue.

It might be that for some edge cases, I use a more powerful model and it's able to figure it out. Tests would need to be established to get to this point but it's a fun thing to think about. I'm probably going to work on this today and mess around with OpenRouter. Previously I would have jumped my way to OpenAI and I do believe the models there are the best. Productsbolt doesn't need that though. it needs enough and testing will help get to that point. That being said, OpenAI have a generous free usage, so I'll think about it but it just seems cool to use OpenRouter for now.

### Conclusion

I'm having a hard time thinking where to optimise next. There's probably somewhere any it annoys me I can't figure out where. There's the 404 pages still to look at but I think the problem is so small, other things can be looked at. I think it'll be cool to see where other models can help and see how much it is. If models costs can be extremely low, but the performance jumps massively, then I'll think of ways to add it in when I absolutely know Qwen has got it wrong. This though can only happen if there's money coming in but I'm in the thought process that it'll only take a very small amount of customers to pay for resources which will essentially be good enough. However if that never happens, I want Productsbolt to be there for those that want cheaper products that aren't easily found thanks to terrible SEO or bad management.

Right let's test some models GO GO GO GO.
